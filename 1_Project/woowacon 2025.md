---
title:
date: 2025-10-28T10:24:50+09:00
aliases:
tags:
  - conference
  - woowabros
description:
updated: 2025-10-28T19:22
---

## Overview

- 날짜: 2025-10-28 10:00~18:00
- 장소: 인터컨티넨탈 파르나스 #hotel

## Contents

### 배달 로봇을 위한 맵 서비스: 강남 한복판에서 달리는 법

#robot #map #robotics

#### Why

- Track F
- 로봇배달이 가능한 곳에서는 로봇배달 배너가 표시됨
- Dilly 라는 로봇이 집 앞까지 찾아옴
    - 완전 자율주행
- why, expandation
- 기존의 지도 서비스는 로봇을 위한 지도가 아니였음
- 주차장 입구는 차량 출입이 가능함
    - 벨지안로 = 울퉁불퉁한 벽돌로 이루어진 도로
        - 노면에서 전해지는 충격이 강하기 때문에 민감한 로봇에겐 좀 걱정
        - 그래도 딜리는 잘 지나가긴 함
    - 볼라드와 연석 = 주행이 가능한 폭인지 판단해야 함
- 세세한 정보를 가져야 로봇이 사용할 수 있음
    - 맵 서비스를 만들어야 하는 이유 1
- Localization 필요성
    - 딜리가 정확히 어디에 있는지 알아내는 방법
    - GPS 센서가 있지만 오차가 심함
    - LiDAR 센서 - 주변을 3D 로 인식
    - 사전에 Point Cloud data 와 실시간으로 수집되는 데이터를 비교하여 정확한 위치를 구할 수 있음
- 그러나 PCD 데이터는 대용량 데이터이고 전국의 데이터를 들고 다닐 수는 없음
    - 딜리에게는 주행에 필요한 타일맵만 제공
    - 맵 서비스가 필요한 이유 2
- 반응형 사물
    - 신호등, 출입문, 엘리베이터 등 인지하고 반응해야 하는 사물
    - 신호등을 큰 덤프트럭이 막고 있다면, 인지모델로는 신호를 확인할 수 없음
- 맵 서비스로 받은 이동경로에 있는 신호등 위치와 아이디를 기반으로 차세대 지능형 교통시스템에서 정보를 얻어 주행을 지속할 수 있게 함
    - 맵 서비스가 필요한 이유 3
- 주행 데이터 축적 -> 주행 성능 고도화

---

#### How. 맵 서비스를 어떻게 만들었는가?

- 초기에는 미터단위로 관리하는 로컬 좌표계로도 충분
    - 준비된 PCD 의 임의의 지점을 원점으로 지정하는 방식
    - 지정한 원점과 지도의 형태에 따른 변환 공식이 필요
- 글로벌 좌표계 도입
    - EPSG:4326
    - 통신망, 관제 시스템 등 외부 데이터와 연동 가능
- 맵 데이터 정의
    - 애로사항: 좁은 폭, 파손된 인도, 부담스러운 연석, 벅찬 경사, 주차장 입구, 그리고 모든 것의 조합
    - 상황 분류보다는 도로의 지리적 특성에 집중
        - 노드
        - 링크: 노드를 잇는 경로
        - POI
        - 가상벽: 안전한 주행을 보장하기 위한 선으로 된 가상의 벽
- 맵 데이터 제작
    - point cloud 생성
    - 색 입히기
    - 글로벌 좌표계에 정렬
    - 맵 어노테이션 생성
    - 타일맵 생성
- 맵 서비스 API 서버 개발

---

#### Next. 확장성을 높이기 위한 과제들

- 서울 전지역, 전국에서 서비스를 하기 위해서는 맵 서비스 확장성을 높여야 함
- 구역이 확장되면 유지보수 리소스 증가
- 유지보수 리소스를 줄이기 위한 노력
    - 주행 중 수집된 Point cloud 를 PCD 업데이트에 사용
    - 관제사 개입 기록
- 업그레이드 된 딜리
    - 드론 or 4족 보행이 된다면?

### 진화하는 스트리밍 플랫폼: 견고함을 향한 여정과 인사이트

- Phase 1
- 실시간에 대한 요청이 점점 늘어남
    - 분석 등
- 배치 엔진으로 spark 을 활용하고 있었고, 자연스럽게 스트리밍에도 spark 로 결정
- streaming vs structured streaming
    - structured streaming 이 커뮤니티에서 권장되는 사항이기 때문에 그 쪽으로 결정
- [[Airflow]] 를 특별한 목적을 위해 사용 중
    - 외부 테이블 데이터를 참조해야하는 상황이 종종 있었기 때문
- 갱신된 테이블을 참조하려면
    - **앱 재시작** 에 airflow 를 사용 주기적으로
    - 외부 저장소 이용
    - 주기적 업데이트
- Phase 2
- 정부가 추진하는 마이데이터 사업을 지원하기 위한 실시간 파이프라인 구성 요구
- phase 1 의 한계
    - phase 1 에서는 인프라와 강결합 구조
    - 관리의 복잡성
    - 스트리밍 앱임에도 매일 재시작이 필요 = 비효율
    - scale in/out 미흡
- Spark vs **Flink** 를 비교해봄
    - Flink 는 태생부터 스트리밍을 위해 탄생함
    - Spark 는 배치로 시작
    - 업계에서도 Flink 로 스트리밍을 처리하는 추세
- 운영 부담을 줄이기 위해 managed service 를 사용하기로 함
    - Managed Apache Flink
- 파이프라인이 증가하며 새로운 문제가 발생함
    - 가장 큰 문제는 비용 문제
    - 제한적인 유연성
    - 최신 버전 도입 지연
    - 디버깅의 어려움
- 기존 시스템의 문제점이 명확했기 때문에 새로운 방법을 시도
- Phase 3
    - Managed Flink 에서 [[Kubernetes]] 기반의 전환을 시작
- 뭐가 좋아지나?
    - 운영비용 감소
    - spot 인스턴스 활용
    - 완전한 운영 제어권
        - 최신 flink 버전 사용 가능
    - 향상된 개발과 배포 경험
    - CI/CD 파이프라인 통합
- Flink 는 [[Kubernetes]] operator 를 이미 지원해서 쉽게 전환이 가능했음
    - 전체 마이그레이션 진행
    - 초기 [[Spark]] 파이프라인 역시 flink 로 전환
- 결과적으로 62% 의 비용 절감
- 운영 안정성 강화 작업 시작
    - schema registry 도입
    - 잘못된 이벤트가 발행되면 애플리케이션이 다운되는 문제가 있었음
- 스키마 레지스트리의 장점
    - 데이터 품질 보장
    - 안정성 극대화
    - 데이터 전송 효율 향상
        - Avro, protobuf 의 장점
- DLQ 도입
    - DLQ 에 메세지가 들어오면 이벤트를 발행한 도메인팀에 즉시 알림
- Count validation 시스템 도입
    - 건수를 실시간으로 체크
    - 이상수치가 감지되면 알람 발송
- [[Grafana]] 대시보드 기반 alerting 구성
- 이벤트 재처리 및 백업
    - 이벤트를 재발행하기 어려운 상황을 대비하여 이벤트 백업
        - S3
        - 원본 토픽 확인에도 유리
    - 별도의 백업 파이프라인 구성
- 백업 파이프라인은 kafka connect 를 활용
    - 손쉽게 iceburg 포맷으로 저장 가능
- StarRocks 도입
    - 실시간 데이터분석 서비스
- 스트리밍 플랫폼 - 우아한플링크
    - yml 과 sql 만 가지고도 애플리케이션을 개발할 수 있도록 설계
    - 로직이 복잡하다면 - UDF 인터페이스 제공
    - 노트북 환경을 직접 제공
        - Zeppelin
    - Flink dashboard and Grafana dashboard 제공
    - 현재까지 5팀 도입
- 향후 목표
    - 스트리밍 레이크하우스 강화
    - 현재는 아이스버그, 미래는 아파치 파이몬? 기반 사용 예정
    - Managed service 수준의 서비스 제공
    - 실시간 검증 시스템 고도화
- QnA
    - 스키마 레지스트리를 사용했을 때 잘못된 메세지가 발행된 경우 처리에 대한 질문
        - 잘못되었더라도 모든 메세지는 저장
        - 메세지가 잘못 발행되면, 해당 도메인팀에 알림을 줘서 이벤트를 다시 발행하도록 했음
    - count validation 의 정합성 검증 방법에 대해
        - 한 번만 검증하지 않고, 분단위 검증, 일단위 검증 등등 윈도우를 여러 단계로 겹쳐서 여러 번 검증함

---

### 데이터 연금술

- 커머스 데이터 허브를 운영 중
    - 하루에 10억건
    - 분당 5만회 조회
    - 여러 출처로부터 데이터 원본 전달받기
    - Data lake + warehouse
- 태스크가 늘어나면서 DAG 이 뒤죽박죽이 됨
    - 어디를 수정해야 원하는 기능을 구현할 수 있을지 알기 어려워짐
    - 실수 확률이 높아짐
    - 수정 사항의 무한 전파
    - 최종 변경점 부하
- 레이어 분리로 해결할 수 있지 않을까?
- **메달리온 레이크하우스 아키텍처**를 알게 되고 스터디를 진행
    - 데이터 품질에 따라, 브론즈 실버 골드 로 구분해서 관리함
    - 제약 사항을 설정
    - 데이터브릭스에서 제안한 개념
    - 원형의 데이터를 비즈니스에 필요한 형태로 가공한다는 유사점
- 아키텍처 재정의
    - 메달리온 아키텍처를 실정에 맞게 정의
    - collect - valid and optimization - business and presentation
    - bronze 의 스키마를 조회팀이 직접 바라보지 못하게
    - 조회팀은 silver 부터 볼 수 있게 한다
    - bronze 는 원본을 그대로 저장
    - silver 에서는 bronze 로부터 넘어오게 조치
    - silver 는 원장 데이터를 조합할 수 있음
    - gold 는 비즈니스 로직 구현
- AGENTS.md 에 각 layer 에 알맞은 코드를 작성해줄 수 있었음
- 지점 상품 Fanout 문제
    - 메달리온을 적용하고 스키마를 분리하여 해소
- 앞으로 하고 싶은 것
    - 레이어 간 인터페이스 혹은 모듈을 통한 코드 강제
    - 레이어를 기준으로 DAG 진행 상황을 한눈에 확인할 수 있는 모니터링 대시보드 구성

현재 LBS Engineering 에서 하고 있는 고민과 매우 유사했고, 구현한 부분이나 앞으로의 계획도 비슷한 느낌

### ServerSentEvents 로 실시간 알림 전송하기

- 기존 알림 시스템의 문제
    - Zero payload 로 인하여 업데이트를 즉각 반영하기 어려웠음
    - 부실한 보안 적용 - zero payload 였기 때문에 유의미한 정보를 얻기 어려웠기 때문에 큰 문제가 아니였음
    - 네트워크 방화벽 차단
    - 웹뷰에서 연결이 어려움
- AWS IoT 도입
    - 메세지 보안 강화
    - 메세지 정형화
- SSE 도입
- 메세지 전달 아키텍처
    - 모든 서버로 메시지 전송
    - 네트워크 홉이 증가하더라도 메시지 전송이 보장되고 유실가능성이 적기 때문
- [[Spring WebFlux|WebFlux]] + Corooutine 환경
- 전송한 메시지를 일정 기간 보관하여 순단이 생기더라도 메시지를 유실하지 않도록 구성
- 주문 요청, 접수 등 중요한 메시지의 경우 정확히 1번 메시지 처리를 구현해야 했음
    - 메세지 페이로드에 url 을 포함하고 해당 url 이 호출되면 DB 상태 update
- 메시지 보안
    - 같은 세션 동시 접속 방어
- 클라이언트 Polling 제거
- 백프레셔 문제
    - Consumer poll timeout has expired.
    - kafka max poll interval 초과로 파티션 할당 해제
    - 코루틴 채널에 문제가 있었음
        - 랑데뷰가 0 이 기본값이라, 바로 연결이 되지 않으면 에러
    - onBufferOverflow 설정도 기본값인 SUSPEND 로 동작 중
- 배포할 때 일정 주기로 thundering herd 문제가 발생
    - 재연결 타이밍을 조절
- SSE 도입 이점

### 무조건 격리한다고 좋은 테스트일까? MSA 환경에서 구축하는 시나리오 인수 테스트

- 연결된 흐름을 테스트할 수 있는 방법은 없을까?
- 73.5% coverage, 3.4k 단위테스트로도 여전히 불안함
- 앱들의 조합으로 구성된 MSA 에서는 단위테스트가 연결성을 검증해줄 수 없다
- 시나리오를 먼저 작성하고 cucumber 를 사용하여 yaml 에 매핑
- 해피 케이스를 중심으로 인수 조건을 작성
- 해피 케이스가 안정화된 이후, 배포를 거듭하며 필요한 엣지 케이스를 리그레션 테스트로 추가
- MSA 에서 어떻게 테스트 환경을 만들 수 있는가?
    - 도커로 전부 말아서 한 번에 띄워봄
    - 테스트 환경이 구축되어 있는지 단계적으로 검증해야 한다
- smoke 테스트
    - 서버가 정상적으로 실행이 되었는가를 확인하는 단계
    - health check
- E2E 테스트
    - 서비스들이 잘 연결되어 호출하고 있는가를 확인하는 단계
- Acceptance Test
    - 일반적인 유닛 테스트
- 테스트 관리를 편하게 하기 위해 AI 를 적극적으로 활용하는 방법을 시도 중
- 우리팀은 리소스가 없는데?
    - 환경은 주어지는게 아니라, 만들어나가는 것
- 점진적 개선이 중요
    - 리소스가 많다고 한 번에 변화를 시도하면 대부분은 어려움을 겪는다

### 외부 벤더 연동, Mediator 시스템

- 미디에이터란?
    - 중재자, 내부 시스템과 외부 벤더의 중재를 맡는 시스템
    - 예시) 맵 미디에이터
- 미디에이터가 되기까지
    - 동일한 기능을 하는 시스템의 중복
- 벤더 연동의 책임을 한 시스템에게 맡기자
    - 일종의 게이트웨이
    - 그러나, 벤더 장애는 전체 장애로 이어진다
    - 시스템 성능 = 벤더 성능
    - 비용 협상력 저하
- 하나의 벤더만 연동해서는 답이 없다
    - 가용성 확보
    - 추가 성능 확보
    - 가격 협상력 확보
    - 위 목적을 위해서 다중 벤더 연동이 필요
- 멀티 벤더 게이트웨이가 되면 반드시 필요한 것들
    - 트래픽 분배
    - 장애 대응
    - 데이터 통합
    - 비용 관리
- 이런 기능을 제공하는 멀티 벤더 게이트웨이를 미디에이터라고 부르고 있음
- 라우팅 시스템
    - 벤더 별로 특성이 제각각
    - 특성을 고려한 가중치 기반 트래픽 분배 필요
    - 장애 감지 시, 준 실시간 트래픽 재분배 필요
    - 맵 미디에이터에서 이 기능을 제공
        - [[Circuit Breaker]]
        - [[Rate Limiter]]
        - 사전에 정의된 비율로 트래픽 전환
- 장애 대응 시스템
    - 일시적 장애시 Fallback, 지속적 장애시 fail over
    - 맵 미디에이터
        - 각 벤더의 응답을 실시간(동기식)으로 받는 구조
        - 특정 벤더의 일시적 오류 발생시 다음 벤더를 통해 응답
            - 아마 fallback 함수 구현했을 듯
            - [[Circuit Breaker]]
        - 오픈소스엔진 도입을 통한 서비스 지속성 보장
    - 메시지 미디에이터
        - 동기와 비동기가 섞여있는 플로우가 있기 때문에, 동작방식에 맞춘 fallback 로직 구현
        - 벤더사로 요청시 오류가 발생할 경우
            - 재처리 큐 사용
        - 전체 벤더 실패 시 배치를 통해 재발송 기능 제공
        - 벤더사 발송 처리 중 장애 대응
            - 수신확인 데이터가 적재 되지 않을 경우, 장애로 판단하고 트래픽 전환
- 데이터 통합 시스템
    - 벤더마다 페이로드는 각기 다름
    - 일관된 형태로 제공하기 위한 시스템
    - Raw -> Data Normalizer -> Data Bus -> Storage
    - 데이터레이크처럼 활용하기 위해 발행되는 토픽에 kafka connector 를 사용하여 이벤트를 ES 로도 적재
- 사용량 추적 시스템
    - 각각의 벤더사와의 비용 책정 논란을 최소화하기 위한 시스템
    - 벤더사로 메세지가 발행될때마다 집계
    - 호출 제한을 통한 비용 관리
        - 벤더별로 가격정책이 다르기 때문
        - 벤더별 최저 비용 유지
- 미디에이터 시스템 성과 소개
    - 맵 미디에이터의 경우 매월 3억건 이상 호출
    - 메시지 미디에이터는 매월 10억 건 이상 메시지 발송
    - 1분 사이 1건이라도 실패하면 1분은 실패
        - 측정시 가용성 99.998% 유지 중
    - 메시지 미디에이터
        - 99.9997% 발송 성공 보장
    - 호출량과 클라이언트는 늘어나지만 비용은 큰 차이없이 안정적으로 유지 중
- QnA
    - 벤더별로 다른 포맷을 어떻게 복잡하지 않게 관리하는지?
        - 복잡해지는게 맞다. 대신 복잡도를 한 곳으로 모아서 서비스 전체적으로 안정적이 되도록 하고 있다.
    - 다음 스텝은?
        - 장애 전환 시스템이 아직 사람 손을 타야하는 부분이 있다.
        - 그런 부분에 대한 고민을 하고 있음

## Conclusion

- 전반적으로 LBS 와 매우 비슷한 고민이 있었고, 적절한 방향을 잘 찾아간 것 같음
    - 앞으로 구현 방향을 참고하는데 있어서 매우 큰 도움이 됨
- [[Kafka schema registry]] 의 경우는 LBS 에서 검토했었던 바와 같이, 전문 데이터 팀이 다뤄주고 도메인 팀의 요구사항을 처리하는 방향이 맞아보임
    - LBS 팀의 Place 파이프라인에는 다른 도메인 팀의 이벤트가 발행되는 일이 없음
- 특히 메달리온 아키텍처는 이름만 들어보고 진지하게 살펴볼 생각은 안했었는데, 실제 구현 케이스를 확인하니 살펴볼 가치가 있겠다는 생각이 듬.
